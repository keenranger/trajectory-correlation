{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behind-dover",
   "metadata": {},
   "source": [
    "\n",
    "# Import neccessary modules\n",
    "1 for matrix caculation, 2 for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collect-necklace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:28:46.911953Z",
     "start_time": "2021-03-03T00:28:46.907315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/git/trajectory-correlation/module/in_fun.py:20: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(cur) is 0:\n",
      "/git/trajectory-correlation/module/in_fun.py:30: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(cur) is 0:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from module.input_parser_old import input_parser\n",
    "from module import in_fun\n",
    "from module.torch import BinaryClassifier\n",
    "\n",
    "\n",
    "beacon_list = [f\"beacon{idx + 1}\" for idx in range(9)]\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "false-missouri",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:28:55.474585Z",
     "start_time": "2021-03-03T00:28:55.470918Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-detroit",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "python에서 csv파일을 읽어올때, 숫자만 있는것이 아닌 문자열이 포함된 데이터를 읽을 경우 느려지는 것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5796033b-c3d0-43f5-a6d3-c8ea16ddbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train/infra_free/\"\n",
    "room_list = [room + '/' for room in os.listdir(data_path)]\n",
    "room_data_list = []\n",
    "for room in room_list:\n",
    "    file_list = [file for file in os.listdir(data_path + room) if file.endswith(\".csv\")]\n",
    "    cur_room_file = []\n",
    "    for file in file_list:\n",
    "        cur_room_file.append(pd.read_csv(data_path + room + file, header=None).to_numpy())\n",
    "    room_data_list.append(np.concatenate((cur_room_file), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403e7972-f580-4725-9172-3e719535d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 101)\n",
      "(22, 101)\n",
      "(22, 101)\n",
      "(22, 101)\n"
     ]
    }
   ],
   "source": [
    "for room_data in room_data_list:\n",
    "    print(np.shape(room_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-assessment",
   "metadata": {},
   "source": [
    "# Data 만들기\n",
    "같은 방 데이터 끼리 매칭시키고, 다른 방 데이터 끼리 매칭시키고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abstract-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2128, 40)\n"
     ]
    }
   ],
   "source": [
    "matched_same = []\n",
    "for room_data in room_data_list:\n",
    "    matched_same.append(in_fun.train_matcher(room_data[:,1:41], room_data[:,1:41]))\n",
    "same_diff_ble = np.concatenate([same[0] for same in matched_same])\n",
    "same_each_ble = np.concatenate([same[1] for same in matched_same])\n",
    "print(np.shape(same_each_ble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arctic-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2112, 40)\n"
     ]
    }
   ],
   "source": [
    "matched_diff = []\n",
    "for idx in range(len(room_data_list)):\n",
    "    if idx == 0:\n",
    "        matched_diff.append(in_fun.train_matcher(room_data_list[-1][:,1:41], room_data_list[idx][:,1:41]))\n",
    "    else:\n",
    "        matched_diff.append(in_fun.train_matcher(room_data_list[idx-1][:,1:41], room_data_list[idx][:,1:41]))\n",
    "    \n",
    "diff_diff_ble = np.concatenate([diff[0] for diff in matched_diff])\n",
    "diff_each_ble = np.concatenate([diff[1] for diff in matched_diff])\n",
    "print(np.shape(diff_each_ble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-decimal",
   "metadata": {},
   "source": [
    "# Loading data for nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "checked-reminder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:29:00.210922Z",
     "start_time": "2021-03-03T00:29:00.153577Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor_diff = torch.from_numpy(np.concatenate((same_diff_ble, diff_diff_ble))).float()\n",
    "x_tensor_each = torch.from_numpy(np.concatenate((same_each_ble, diff_each_ble))).float()\n",
    "\n",
    "y = np.concatenate((np.zeros([len(same_each_ble), 1]) + 1, np.zeros([len(diff_each_ble), 1])))\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset_diff = TensorDataset(x_tensor_diff, y_tensor)\n",
    "dataset_each = TensorDataset(x_tensor_each, y_tensor)\n",
    "\n",
    "train_len_diff = len(dataset_diff) // 5 * 4\n",
    "train_len_each = len(dataset_each) // 5 * 4\n",
    "val_len_diff = len(dataset_diff) - train_len_diff\n",
    "val_len_each  = len(dataset_each) - train_len_each\n",
    "\n",
    "train_dataset_diff, val_dataset_diff = random_split(dataset_diff, [train_len_diff, val_len_diff])\n",
    "train_dataset_each, val_dataset_each = random_split(dataset_each, [train_len_each, val_len_each])\n",
    "\n",
    "train_loader_diff = DataLoader(dataset=train_dataset_diff, batch_size=32, shuffle=True)\n",
    "train_loader_each = DataLoader(dataset=train_dataset_each, batch_size=32, shuffle=True)\n",
    "val_loader_diff = DataLoader(dataset = val_dataset_diff, batch_size = val_len_diff)\n",
    "val_loader_each = DataLoader(dataset = val_dataset_each, batch_size = val_len_each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-punishment",
   "metadata": {},
   "source": [
    "# Layer\n",
    "nn 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collectible-overall",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:29:00.253558Z",
     "start_time": "2021-03-03T00:29:00.219370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documentary-butter",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-03T00:29:15.426Z"
    }
   },
   "outputs": [],
   "source": [
    "net_diff = BinaryClassifier(input_size = 20)\n",
    "net_each = BinaryClassifier(input_size = 40)\n",
    "net_diff.to(device)\n",
    "net_each.to(device)\n",
    "optimizer_diff = optim.SGD(net_diff.parameters(), lr=3e-3)\n",
    "optimizer_each = optim.SGD(net_each.parameters(), lr=3e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-complaint",
   "metadata": {},
   "source": [
    "비교분석을 위한 수치  \n",
    "모든 데이터에 true라고 응답해도, 정확도가 62%는 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "assumed-source",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-03T00:30:53.847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5018867924528302\n"
     ]
    }
   ],
   "source": [
    "print(len(same_each_ble) / (len(same_each_ble) + len(diff_each_ble)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "maritime-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 1, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 2, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 3, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 4, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 5, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 6, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 7, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 8, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 9, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 10, Val Accuarcy : 49.410377358490564%\n",
      "epoch: 11, Val Accuarcy : 50.0%\n",
      "epoch: 12, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 13, Val Accuarcy : 60.37735849056604%\n",
      "epoch: 14, Val Accuarcy : 66.50943396226415%\n",
      "epoch: 15, Val Accuarcy : 72.75943396226415%\n",
      "epoch: 16, Val Accuarcy : 77.7122641509434%\n",
      "epoch: 17, Val Accuarcy : 80.54245283018868%\n",
      "epoch: 18, Val Accuarcy : 83.9622641509434%\n",
      "epoch: 19, Val Accuarcy : 87.02830188679245%\n",
      "epoch: 20, Val Accuarcy : 88.56132075471697%\n",
      "epoch: 21, Val Accuarcy : 90.33018867924528%\n",
      "epoch: 22, Val Accuarcy : 91.50943396226415%\n",
      "epoch: 23, Val Accuarcy : 92.80660377358491%\n",
      "epoch: 24, Val Accuarcy : 93.39622641509435%\n",
      "epoch: 25, Val Accuarcy : 93.86792452830188%\n",
      "epoch: 26, Val Accuarcy : 94.33962264150944%\n",
      "epoch: 27, Val Accuarcy : 94.5754716981132%\n",
      "epoch: 28, Val Accuarcy : 94.92924528301887%\n",
      "epoch: 29, Val Accuarcy : 95.40094339622641%\n",
      "epoch: 30, Val Accuarcy : 95.40094339622641%\n",
      "epoch: 31, Val Accuarcy : 95.63679245283019%\n",
      "epoch: 32, Val Accuarcy : 95.75471698113208%\n",
      "epoch: 33, Val Accuarcy : 95.99056603773585%\n",
      "epoch: 34, Val Accuarcy : 95.99056603773585%\n",
      "epoch: 35, Val Accuarcy : 95.99056603773585%\n",
      "epoch: 36, Val Accuarcy : 96.34433962264151%\n",
      "epoch: 37, Val Accuarcy : 96.4622641509434%\n",
      "epoch: 38, Val Accuarcy : 97.16981132075472%\n",
      "epoch: 39, Val Accuarcy : 97.05188679245283%\n",
      "epoch: 40, Val Accuarcy : 97.2877358490566%\n",
      "epoch: 41, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 42, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 43, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 44, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 45, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 46, Val Accuarcy : 97.52358490566037%\n",
      "epoch: 47, Val Accuarcy : 97.52358490566037%\n",
      "epoch: 48, Val Accuarcy : 97.52358490566037%\n",
      "epoch: 49, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 50, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 51, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 52, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 53, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 54, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 55, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 56, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 57, Val Accuarcy : 97.99528301886792%\n",
      "epoch: 58, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 59, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 60, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 61, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 62, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 63, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 64, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 65, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 66, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 67, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 68, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 69, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 70, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 71, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 72, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 73, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 74, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 75, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 76, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 77, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 78, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 79, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 80, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 81, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 82, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 83, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 84, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 85, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 86, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 87, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 88, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 89, Val Accuarcy : 98.82075471698113%\n",
      "epoch: 90, Val Accuarcy : 99.29245283018868%\n",
      "epoch: 91, Val Accuarcy : 99.1745283018868%\n",
      "epoch: 92, Val Accuarcy : 99.05660377358491%\n",
      "epoch: 93, Val Accuarcy : 99.1745283018868%\n",
      "epoch: 94, Val Accuarcy : 99.1745283018868%\n",
      "epoch: 95, Val Accuarcy : 99.1745283018868%\n",
      "epoch: 96, Val Accuarcy : 99.1745283018868%\n",
      "epoch: 97, Val Accuarcy : 99.29245283018868%\n",
      "epoch: 98, Val Accuarcy : 99.41037735849056%\n",
      "epoch: 99, Val Accuarcy : 99.41037735849056%\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_idx, data in enumerate(train_loader_diff):\n",
    "        x_train, y_train = data[0].to(device), data[1].to(device)\n",
    "        hypothesis = net_diff(x_train)\n",
    "        cost = criterion(hypothesis, y_train)\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer_diff.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer_diff.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader_diff:\n",
    "            x_val, y_val = data[0].to(device), data[1].to(device)\n",
    "            hypothesis = net_diff(x_val)\n",
    "            prediction = hypothesis >= torch.FloatTensor([0.5]).to(device)\n",
    "            correct_prediction = prediction.float() == y_val  # 실제값과 일치하는 경우만 True로 간주\n",
    "            val_accuarcy = correct_prediction.sum().item() / len(correct_prediction)  # 정확도를 계산\n",
    "            print(f\"epoch: {epoch}, Val Accuarcy : {val_accuarcy * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proved-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 1, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 2, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 3, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 4, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 5, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 6, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 7, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 8, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 9, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 10, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 11, Val Accuarcy : 53.18396226415094%\n",
      "epoch: 12, Val Accuarcy : 53.655660377358494%\n",
      "epoch: 13, Val Accuarcy : 56.95754716981132%\n",
      "epoch: 14, Val Accuarcy : 62.264150943396224%\n",
      "epoch: 15, Val Accuarcy : 67.80660377358491%\n",
      "epoch: 16, Val Accuarcy : 74.41037735849056%\n",
      "epoch: 17, Val Accuarcy : 79.95283018867924%\n",
      "epoch: 18, Val Accuarcy : 83.37264150943396%\n",
      "epoch: 19, Val Accuarcy : 86.43867924528303%\n",
      "epoch: 20, Val Accuarcy : 88.91509433962264%\n",
      "epoch: 21, Val Accuarcy : 90.2122641509434%\n",
      "epoch: 22, Val Accuarcy : 92.45283018867924%\n",
      "epoch: 23, Val Accuarcy : 94.22169811320755%\n",
      "epoch: 24, Val Accuarcy : 94.45754716981132%\n",
      "epoch: 25, Val Accuarcy : 95.04716981132076%\n",
      "epoch: 26, Val Accuarcy : 95.40094339622641%\n",
      "epoch: 27, Val Accuarcy : 95.75471698113208%\n",
      "epoch: 28, Val Accuarcy : 96.22641509433963%\n",
      "epoch: 29, Val Accuarcy : 96.22641509433963%\n",
      "epoch: 30, Val Accuarcy : 96.69811320754717%\n",
      "epoch: 31, Val Accuarcy : 97.16981132075472%\n",
      "epoch: 32, Val Accuarcy : 97.2877358490566%\n",
      "epoch: 33, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 34, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 35, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 36, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 37, Val Accuarcy : 97.40566037735849%\n",
      "epoch: 38, Val Accuarcy : 97.64150943396226%\n",
      "epoch: 39, Val Accuarcy : 97.52358490566037%\n",
      "epoch: 40, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 41, Val Accuarcy : 97.75943396226415%\n",
      "epoch: 42, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 43, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 44, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 45, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 46, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 47, Val Accuarcy : 97.87735849056604%\n",
      "epoch: 48, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 49, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 50, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 51, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 52, Val Accuarcy : 98.11320754716981%\n",
      "epoch: 53, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 54, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 55, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 56, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 57, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 58, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 59, Val Accuarcy : 98.23113207547169%\n",
      "epoch: 60, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 61, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 62, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 63, Val Accuarcy : 98.34905660377359%\n",
      "epoch: 64, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 65, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 66, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 67, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 68, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 69, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 70, Val Accuarcy : 98.46698113207547%\n",
      "epoch: 71, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 72, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 73, Val Accuarcy : 98.58490566037736%\n",
      "epoch: 74, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 75, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 76, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 77, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 78, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 79, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 80, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 81, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 82, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 83, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 84, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 85, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 86, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 87, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 88, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 89, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 90, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 91, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 92, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 93, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 94, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 95, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 96, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 97, Val Accuarcy : 98.70283018867924%\n",
      "epoch: 98, Val Accuarcy : 98.93867924528303%\n",
      "epoch: 99, Val Accuarcy : 98.82075471698113%\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_idx, data in enumerate(train_loader_each):\n",
    "        x_train, y_train = data[0].to(device), data[1].to(device)\n",
    "        hypothesis = net_each(x_train)\n",
    "        cost = criterion(hypothesis, y_train)\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer_each.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer_each.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader_each:\n",
    "            x_val, y_val = data[0].to(device), data[1].to(device)\n",
    "            hypothesis = net_each(x_val)\n",
    "            prediction = hypothesis >= torch.FloatTensor([0.5]).to(device)\n",
    "            correct_prediction = prediction.float() == y_val  # 실제값과 일치하는 경우만 True로 간주\n",
    "            val_accuarcy = correct_prediction.sum().item() / len(correct_prediction)  # 정확도를 계산\n",
    "            print(f\"epoch: {epoch}, Val Accuarcy : {val_accuarcy * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-round",
   "metadata": {},
   "source": [
    "# 개선 계획\n",
    "학습 과정에서 accuracy가 높아지지 않은 것으로, 제대로 학습되지 않는 상태인것으로 확인  \n",
    "input에 대한 조절로 결과를 얻고자한다\n",
    "1. RSSI 값 연산 시 재정의:   \n",
    "현재 해당 비컨에 해당하는 RSSI가 없으면, 0으로 넣어주고 비교하고있다.  \n",
    "그결과, null데이터와 -90dB의 차이가, null데이터와 -10dB의 차이보다 더 큰 상태이다.  \n",
    "null값에 대해 -100을 넣어주고 연산하는 방식으로 오류를 줄여야 한다.  \n",
    "2. Input 범위 재설정:  \n",
    "1의 적용 이후, 실제 input으로 들어갈 값들은 -100 ~ -1 사이의 값이다.  \n",
    "input에 100을 더한 후 100으로 나눠주어, input들이 0 ~ 1 사이의 범위에 있도록 normalize 해줄 계획이다\n",
    "3. Input Data 추가:  \n",
    "현재는 두 데이터간의 '차의 절댓값' 만을 넣어주고 있어서, 데이터 dimension이 9이하인 상태이다. (적은 편)  \n",
    "차만을 사용하는 과정에서, 각 데이터의 RF measurement들이 버려지게 된다.  \n",
    "4. Input의 다원화:  \n",
    "현재 사용하고 있지 않은 기압값등을 어떻게 활용할지 고려한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "western-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_diff.state_dict(), \"./model/diff\")\n",
    "torch.save(net_each.state_dict(), \"./model/each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-allocation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
